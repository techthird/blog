# 实战！火星系统并发从1万到10万的升级之路
---

# 背景
从2021年10月份开始，火星业务在明星代言人的加持下，品牌效应越来越深入人心，业务的增长也达到了前所未有的高度。
可想而知，对系统的考验也是指数级的上升，当前系统还处于单体的应用架构，很难及时应对业务的快速增长，导致问题频发，影响用户体验、阻碍活动效果。

技术存在的意义，是通过业务来体现价值，当无法稳定支撑业务发展时，系统的问题急需解决。
我们联合研发、架构、运维、DBA，成立了系统攻坚小组，从不同的维度，评估出不同的优化点，并给出有建设性意义的解决方案。

目标统一，配合默契，经过1.5个月的奋战，我们完成了系统并发从1万到10万的升级之路。

# 优化思路
- 确定压测目标：
  - 读写混合：10WQPS
  - 创建招募订单：2000TPS
- 整理主流程接口构造场景压测（首页、商品列表、商详、订单、用户等接口）。
- 使用async-profiler工具优先分析堆栈、内存分配。
![2022428114843.png](https://restest.sx.ink/2022428114843.png)

- 通过grafana监控，抓出耗时最高的接口，逐步攻破
![2022428115539.png](https://restest.sx.ink/2022428115539.png)

- 通过调用链erlang分析耗时最高接口的方法级耗时
![202259102432.png](https://restest.sx.ink/202259102432.png)


- 找到具体问题方向后，发动连环大招，开始透过本质去解决问题
  - 代码走读
  - 分析与思考
  - 制定解决方案
  - 评审方案
  - 代码改造 
  - 代码评审
  - 简单的功能验证
  - 反复压测验证
  - 总结
  - 【循环此步骤，直到问题解决】


# 现有系统架构
- 单体应用
- 90%接口查询走数据库，少量查询缓存
- 定时任务、MQ消费代码耦合
![2022428111048.png](https://restest.sx.ink/2022428111048.png)

### 该架构存在的问题
1、单体应用，扩容有上限，加机器也无法提升系统并发能力。
以腾讯云Mysql举例：8核32G的配置，最大支撑QPS为38952。超过这个阀值将出现服务不可用，宕机的现象。

2、服务模块耦合，一个小功能出现问题，将影响整个系统的稳定性。
如定时任务执行的时候，占用太多的CPU，直接影响商品、用户等功能稳定性。

3、单体应用的其他缺点：版本控制艰难、可维护性差、扩展性差、资源无法隔离和降级。

4、数据库： 90%的查询接口走数据库查询，数据库本身的设计机制就不是应对高并发、大流量的场景。
案例：前端场景95%接口都需要查询用户信息、用户组织关系。
压测时，读写混合8000并发，数据库QPS大概在32000以上。流量翻了4倍。

5、首页（订货中心）：主、次功能未做接口拆分，导致次要功能的查询拖慢整个主场景的加载。

# 优化后的架构

- **分布式：** 拆服务，建立分布式应用架构
- **消费端独立：** MQ消费端独立，减少线程上下文切换
- **定时任务独立：** 规避定时任务执行中，对交易流程的影响
- **缓存：** 前台场景，增加缓存查询，释放数据库压力，提升高并发能力和响应速度
- **本地缓存：** 增本地缓存查询，规避单Key热点问题
- **从库：** 允许短暂延迟的业务，增加从库查询
- **拆库：** 规避单库瓶颈问题
- **接口拆分：** 一个场景页面，拆分主次接口查询
案例：
主接口：订货中心首页菜单、用户信息。
次要接口：本月销售额、今日新增董事数、累计积分。

![202251891248.png](https://restest.sx.ink/202251891248.png)


# 过程中解决的问题

### 高并发库存扣减
系统早期上线，使用Mysql乐观锁的机制进行库存扣减，正常并发能力每秒在100-200之间，超过这个阀值就会产生锁等待，CPU100%的情况，数据库不可用，95%的业务功能都将无法正常进行。

```sql
-- 扣减库存脚本
update t_stock set stock = stock - 1 where barcode = 'bar001' and  stock - 1 >= 0;
```
改造方案使用lua原子脚本扣减库存，原理如图：
<img src="https://restest.sx.ink/20225189395.png" height="650px">


Reids，单线程支持顺序操作，而且性能优异，但是不支持事务回滚。但通过redis+lua脚本可以实现redis操作的原子性,这种方案同时满足顺序性和原子性的要求。

应用在库存扣减的场景中，可以说是完美匹配，原子操作保证不会超卖，顺序性保证扣减的并发能力。

**压测结论：** 4台redis集群，每秒扣减库存可以达到36000每秒，足以支撑任何任何秒杀场景（大厂除外）。

### 用户信息、组织关系缓存
再看下之前的场景8000并发，数据库QPS大概在32000以上，流量翻了4倍。

在电商场景中，任何交易的产生，都依赖用户信息、组织关系数据。如首页的个性化推荐，商详用户级价格等，并且一个场景页面中，一般是多个接口的数据组合成一个完整的页面展示。

用户发起一次请求，对数据库来说，接收到了3次以上的请求，甚至更多。而且每次请求用户信息、组织关系的的入参出参相似度几乎是100%，如此重合度高的接口，做缓存优化，性能大概率会成倍的提升。

得出初步分析结论后，不犹豫，立即开工改造代码。经过一周的时间改造上线，最终呈现的结果不出所料，性能大幅度提升，CPU下降30%。
过去4000qps，服务器cpu至少是35-40%；
2月25日的4000qps，cpu7%。 

![2022512174914.png](https://restest.sx.ink/2022512174914.png)

![2022512174922.png](https://restest.sx.ink/2022512174922.png)

### 销售额统计行锁
问题产生的场景，一个原点同时向多个用户出货的时候，会频繁的执行update脚本累计销售额，示例如下：
```sql
-- 累计销售额脚本
update t_award_month_detail set award  = award + #{award} , amount = amount + #{amount}
where user_id = #{userId} and frame_month = #{frameMonth} and item_id = #{itemId};
```
该脚本存在一个问题，当并发超过每秒200时，会存在锁竞争、锁等待、死锁的情况，严重情况Mysql直接宕机！

**解决方案：**
- MQ消费端，增加批量消费，合并后按批落库
- 调低消费线程数
- 拆库：销售额相关的表独立新库，减少主库的压力
```yml
# MQ消费端参数配置
consumeThreadMin: 5
consumeThreadMax: 5
consumeMessageBatchMaxSize: 32
```

### 接口耗时高，数据库无异常
产生此问题的原因如下：
- 同一台机器，同时提供前端服务、MQ消费服务、任务调度，导致频繁的线程上下文频繁，耗CPU时间。
> 项目线程数如下：
1、48个消费者，每个消费者默认最小消费线程数15，合计720。
2、dubbo线程数：2000
3、定时器调度：34
4、业务代码多线程：100+
总计：4核机器，活动期间，如果全场景功能触发，预估累计线程数为2854+，已经严重超过机器的负载。

**解决方案：**
- 独立MQ消费端，配置指定机器消费。
- MQ消费线程数调整，针对非核心业务，可延迟消费的业务，调低消费线程数。
- 交易主流程，删除不必要的多线程处理逻辑，如订单落库后发送MQ通知、简单查询逻辑删除多线程处理等。
- 独立定时任务调度服务，独立部署。

### 其他问题
- JVM参数设置不合理
```basic
调整前：-Xmx2g -Xms2g -Xmn512m -XX:MetaspaceSize=256m  -XX:MaxMetaspaceSize=256m
调整后：-Xmx4g -Xms4g -Xmn2g -XX:MetaspaceSize=256m -XX:MaxMetaspaceSize=256m
```
- 数据库连接池不生效
- 热点问题解决，增加本地缓存
- 线程数设置不合理
- Mysql索引
- 订单号生成重复，使用分布式ID解决

# 总结

需求紧急度，开发质量在不同阶段、不同情况都会不一样，所以还得有一个核查机制：每次改动主流程场景后，负责人都需要做：
- 技术方案评审
- 代码评审
- 压测验证

系统优化是一个持久战，当前的优化解决了当前的问题，后续的需求迭代中，研发需要具备高并发、大流量的开发意识，主动思考，主动学习积累。

也需要从格局、认知上面去认识到，我写的这个代码会在什么场景用到，当流量巨大的时候，它是否会产生瓶颈。虽然这会带来很多工作量，但作为技术人，这是我们的职责之一，是一种使命、一种信念。


